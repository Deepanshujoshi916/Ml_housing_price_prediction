# -*- coding: utf-8 -*-
"""ml_chapter2 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0AFrfMsFHOxgGD_C9611a1TdZwnsHcx

#GET DATA
"""

import pandas as pd
import os

import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

pwd= os.getcwd()

os.chdir("/content/drive/MyDrive/housing")

df = pd.read_csv('housing.csv')

"""##Transform data"""

df.info()

df['ocean_proximity'].value_counts()

df['ocean_proximity'].value_counts().plot(kind='barh')

df.describe()

df['median_income'].hist()

df['income_cat']=pd.cut(df['median_income'],
                        bins =[0,1.5,3,4.5,6,np.inf],
                        labels=[1,2,3,4,5])
df["income_cat"].value_counts()

df['income_cat'].hist()

df

y = df["median_house_value"]
x = df.drop("median_house_value", axis=1)
x

y

"""##split dataset"""

from sklearn.model_selection import train_test_split

x_train ,x_test,y_train,y_test = train_test_split(x,y,test_size =0.33)

import matplotlib.pyplot as plt

x_train['income_cat'].hist()
income_cat_count = x_train['income_cat'].value_counts().sort_index()
plt.plot(income_cat_count.index,income_cat_count.values,linestyle = '-')
plt.show()

x_train

df["income_cat"].hist()

### stratified test

"""###Stratified Shuffle"""

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1,test_size = 0.2,random_state=42)
for train_index, test_index in split.split(df,df["income_cat"]):
    strat_train_set= df.loc[train_index]
    strat_test_set = df.loc[test_index]

strat_train_set['income_cat'].value_counts()/len(df)

def income_cat_prop(data):
    return data['income_cat'].value_counts()/len(data)

train_set, test_set = train_test_split(df,test_size=0.2,random_state=42)
compare_prop=pd.DataFrame({"overall":income_cat_prop(df),
                          "stratified":income_cat_prop(strat_test_set),
                          "random":income_cat_prop(test_set)}).sort_index()
compare_prop['rand_error']=100*compare_prop["random"]/compare_prop["overall"]-100
compare_prop['strat_error']=100*compare_prop["stratified"]/compare_prop["overall"]-100 #BODMAS

compare_prop

strat_test_set

strat_train_set

strat_train_set.drop("income_cat",axis=1,inplace=True)
strat_test_set.drop("income_cat",axis=1,inplace=True)

"""# DATA VISULIZATION"""

visual= strat_train_set.copy()

visual.hist(bins=50,figsize=(20,15))

visual.hist(figsize=(20,15) )

visual.plot(kind='scatter',x='longitude',y='latitude',s=visual['population']/100,c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True,sharex=False)

numeric = visual.select_dtypes(include=[np.number])
numeric.corr()
a = numeric.corr()

import seaborn as sns
mask = np.triu(np.ones_like(a, dtype=bool))
F,AX = plt.subplots(figsize=(11,9))
cmap=sns.diverging_palette(220,10,as_cmap=True)
sns.heatmap(a,mask = mask,cmap=cmap,vmax=.3,center=0,square=True,linewidths=.5,cbar_kws={"shrink":.5})

"""#Preparing data for ml"""

housing = strat_train_set.drop("median_house_value",axis=1)
housing_labels = strat_train_set["median_house_value"].copy()

"""###Dealing_with_missing_values"""

sample_incomplete_rows= housing[housing.isnull().any(axis=1)].head()
sample_incomplete_rows

sample_incomplete_rows.dropna(subset=["total_bedrooms"])

sample_incomplete_rows.drop("total_bedrooms",axis=1)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

housingnum= housing.select_dtypes(include=[np.number])
housingnum

"""#####loop practicing

"""

triplets = [(1, 'a', "a"), (2, 'b', 'a'), (3, 'c', 's')]

for number, letter, boolean in triplets:
    print(f"Number: {number},  Boolean: {boolean},Letter: {letter}")

imputer.fit(housingnum)

x= imputer.transform(housingnum)
housing_tr= pd.DataFrame(x,columns= housingnum.columns,index = housingnum.index)
housing_tr

from sklearn.preprocessing import OrdinalEncoder
OE = OrdinalEncoder()
housing_cat_encoded = OE.fit_transform(housing[["ocean_proximity"]])
housing_cat_encoded[:10]

housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown="ignore",sparse_output = False).set_output(transform="pandas")
housing_cat_1hot = ohe.fit_transform(housing_cat)
housing_cat_1hot

ohe.categories_

"""##Feature_engineering"""

def feature_engineering(data):
  data["bedroom_per_household"]=data["total_bedrooms"]/data["households"]
  data["rooms_per_thosand"]=data["population"]/data["households"]
  data["rooms_per_household"]=data["total_rooms"]/data["households"]
  return data

housing_feature_eng= feature_engineering(housingnum)
housing_feature_eng

"""###Scaling our data"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().set_output(transform = "pandas")
housing_scaled = scaler.fit_transform(housing_feature_eng)

housing_scaled.head()

"""##Transforming Pipelines"""

housing = strat_train_set.drop("median_house_value",axis=1)
housing_labels = strat_train_set["median_house_value"].copy()
def data_transformations(data):
  if"median_house_value" in data.columns:
    labels= data["median_house_value"]
    data = data.drop("median_house_value",axis=1)
  else :
    labels = None
  feature_engineered_data =feature_engineering(data)
  features = list(feature_engineered_data.columns)

  #median_imputing_data
  from sklearn.impute import SimpleImputer
  imputer = SimpleImputer(strategy="median")
  housing_num = feature_engineered_data.select_dtypes(include =[np.number])
  imputed = imputer.fit_transform(housing_num)

  #encoding categorical data
  housing_cat=feature_engineered_data.select_dtypes(exclude=[np.number])
  from sklearn.preprocessing import OneHotEncoder
  cat_encoder = OneHotEncoder(sparse =False)
  housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
  features = features +cat_encoder.categories_[0].tolist()
  features.remove("ocean_proximity") # as we already encoded this variable
  #scaling numerical data
  from sklearn.preprocessing import StandardScaler
  scaler= StandardScaler()
  housing_scaled = scaler.fit_transform(imputed)
  #concat all data
  output =np.hstack([housing_scaled ,housing_cat_1hot])


  return output , labels,features

train_data, train_labels,features =data_transformations(strat_train_set)
 train_data

test_data, test_labels,features =data_transformations(strat_test_set)
 test_data

"""##Train_models

###Linear_Regressions
"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(train_data,train_labels)

original_values = test_labels[:5]
predicted_values = lin_reg.predict(test_data[:5])

compare = pd.DataFrame(data ={"original_values":original_values,"predicted_values":predicted_values})
compare["difference"]=compare["original_values"]-compare["predicted_values"]

compare

"""###Mean_squared_error and Absolute_error

"""

from sklearn.metrics import mean_squared_error
lin_mse =mean_squared_error(original_values,predicted_values)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

from sklearn.metrics import mean_absolute_error
lin_mae = mean_absolute_error(original_values,predicted_values)
lin_rmae = np.sqrt(lin_mae)
lin_rmae

lin_mse

"""##DecisionTree"""

from sklearn.tree import DecisionTreeRegressor
tree= DecisionTreeRegressor(random_state = 42)
tree.fit(train_data,train_labels)

tree_predict =tree.predict(train_data)
tree_mse = mean_squared_error(train_labels,tree_predict)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree,train_data,train_labels,scoring="neg_mean_squared_error",cv=10)
tree_rmse_scores = np.sqrt(-scores)

def display_scores(scores):
  print("scores:",scores)
  print("mean:",scores.mean())
  print("standard deviation:",scores.std())

display_scores(tree_rmse_scores)

from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor(n_estimators =100,random_state = 42)
forest.fit(train_data,train_labels)

forest_predict =forest.predict(train_data)
forest_mse = mean_squared_error(train_labels,forest_predict)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

from sklearn.model_selection import cross_val_score
scores = cross_val_score(forest,train_data,train_labels,scoring="neg_mean_squared_error",cv=10)
forest_rmse_scores = np.sqrt(-scores)

display_scores(forest_rmse_scores)

"""#FineTune_Model"""

from sklearn.model_selection import GridSearchCV
param_grid = [{"n_estimators":[3,10,30],"max_features":[2,3,4]},
              {"bootstrap":[False],"n_estimators":[3,10],"max_features":[2,3,4]}]
forest_reg = RandomForestRegressor(random_state = 42)
grid_search = GridSearchCV(forest_reg,param_grid,cv=5,scoring="neg_mean_squared_error",return_train_score=True)
grid_search.fit(train_data,train_labels)

grid_search.best_params_

grid_search.best_estimator_

cvrs =grid_search.cv_results_
for mean_score,params in zip(cvrs["mean_test_score"],cvrs["params"]):
  print(np.sqrt(-mean_score),params)

pd.DataFrame(grid_search.cv_results_)

"""##RAndomized_search"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
param_distribs = {"n_estimators":randint(low=1,high=200),
                    "max_features":randint(low=1,high=8)}

forest_reg = RandomForestRegressor(random_state = 42)
rnd_search = RandomizedSearchCV(forest_reg,param_distributions=param_distribs,n_iter=10,cv=5,scoring="neg_mean_squared_error",verbose=2,random_state=42)
rnd_search.fit(train_data,train_labels)

cvres= rnd_search.cv_results_
for mean_score,params in zip(cvres["mean_test_score"],cvres["params"]):
  print(np.sqrt(-mean_score),params)

feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances

feature_importances_list =list(zip(features,feature_importances.tolist()))
feature_importances_list

df1 = pd.DataFrame(sorted (feature_importances_list))

df1.info()

df1 = df1.sort_values(by=1, ascending=False)

plt.figure(figsize=(10,6))
plt.xticks(rotation=75)

plt.bar(df1[0],df1[1])

# prompt: i want to sort above df1 data in desc order

df1 = df1.sort_values(by=1, ascending=False)

sorted_features = sorted(feature_importances_list, key=lambda x: x[1], reverse=True)
features_sorted, importances_sorted = zip(*sorted_features)

plt.figure(figsize=(10, 6))
plt.bar(features_sorted, importances_sorted)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.tight_layout()
plt.show()

